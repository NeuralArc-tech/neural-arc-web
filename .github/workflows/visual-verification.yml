name: Visual Verification with Browser-Use

on:
  workflow_dispatch:
    inputs:
      test_url:
        description: 'URL to test (defaults to deployed site)'
        required: false
        type: string
      viewport_sizes:
        description: 'Viewport sizes to test (comma-separated)'
        required: false
        default: '1920x1080,1366x768,768x1024,375x667'
        type: string
      visual_test_type:
        description: 'Type of visual test'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - regression
          - responsive
          - accessibility

jobs:
  visual-verification:
    name: Visual Verification
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install Browser-Use and dependencies
        run: |
          pip install browser-use playwright opencv-python-headless numpy pillow
          playwright install chromium
          
      - name: Create visual test script
        run: |
          cat > visual_test.py << 'EOF'
          import asyncio
          import os
          import sys
          from datetime import datetime
          from pathlib import Path
          from browser_use import Agent
          from browser_use.browser.browser import Browser, BrowserConfig
          from PIL import Image
          import numpy as np
          import cv2
          
          async def capture_screenshots(url, viewports):
              """Capture screenshots at different viewport sizes"""
              screenshots = {}
              browser_config = BrowserConfig(
                  headless=True,
                  disable_security=True,
                  extra_chromium_args=['--no-sandbox', '--disable-dev-shm-usage']
              )
              
              for viewport in viewports:
                  width, height = map(int, viewport.split('x'))
                  print(f"Capturing screenshot at {width}x{height}")
                  
                  browser = Browser(config=browser_config)
                  await browser.start()
                  
                  page = await browser.get_current_page()
                  await page.set_viewport_size({"width": width, "height": height})
                  await page.goto(url, wait_until="networkidle")
                  
                  # Wait for animations to complete
                  await asyncio.sleep(2)
                  
                  # Capture screenshot
                  screenshot_path = f"screenshots/{viewport}.png"
                  await page.screenshot(path=screenshot_path, full_page=True)
                  screenshots[viewport] = screenshot_path
                  
                  await browser.close()
                  
              return screenshots
          
          async def visual_analysis(url, viewports, test_type):
              """Perform visual analysis using Browser-Use agent"""
              agent = Agent(
                  task=f"""Analyze the website at {url} for:
                  1. Visual consistency across viewport sizes: {', '.join(viewports)}
                  2. Responsive design issues
                  3. Layout problems or overlapping elements
                  4. Typography and spacing issues
                  5. Color contrast and accessibility
                  6. Mobile-specific issues (if applicable)
                  
                  Provide a detailed report with specific issues found and recommendations.
                  """,
                  llm=None  # Will use default model
              )
              
              # Capture screenshots first
              Path("screenshots").mkdir(exist_ok=True)
              screenshots = await capture_screenshots(url, viewports)
              
              # Run agent analysis
              result = await agent.run()
              
              # Generate report
              report = f"""# Visual Verification Report
              
              **URL**: {url}
              **Test Type**: {test_type}
              **Timestamp**: {datetime.now().isoformat()}
              **Viewports Tested**: {', '.join(viewports)}
              
              ## Screenshots Captured
              """
              
              for viewport, path in screenshots.items():
                  report += f"\n### {viewport}\n![{viewport}]({path})\n"
              
              report += f"\n## Analysis Results\n{result}\n"
              
              # Save report
              with open("visual-report.md", "w") as f:
                  f.write(report)
              
              return report
          
          async def compare_screenshots(baseline_dir, current_dir, threshold=0.95):
              """Compare screenshots for visual regression"""
              differences = []
              
              for file in Path(baseline_dir).glob("*.png"):
                  baseline = cv2.imread(str(file))
                  current_path = Path(current_dir) / file.name
                  
                  if current_path.exists():
                      current = cv2.imread(str(current_path))
                      
                      # Calculate similarity
                      gray1 = cv2.cvtColor(baseline, cv2.COLOR_BGR2GRAY)
                      gray2 = cv2.cvtColor(current, cv2.COLOR_BGR2GRAY)
                      
                      score = cv2.matchTemplate(gray1, gray2, cv2.TM_CCOEFF_NORMED)[0][0]
                      
                      if score < threshold:
                          differences.append({
                              "file": file.name,
                              "similarity": score,
                              "status": "changed"
                          })
                      else:
                          differences.append({
                              "file": file.name,
                              "similarity": score,
                              "status": "unchanged"
                          })
              
              return differences
          
          async def main():
              url = sys.argv[1] if len(sys.argv) > 1 else os.environ.get("TEST_URL", "http://localhost:3000")
              viewports = sys.argv[2].split(",") if len(sys.argv) > 2 else ["1920x1080", "768x1024", "375x667"]
              test_type = sys.argv[3] if len(sys.argv) > 3 else "full"
              
              print(f"Starting visual verification for {url}")
              print(f"Viewports: {viewports}")
              print(f"Test type: {test_type}")
              
              report = await visual_analysis(url, viewports, test_type)
              print("\nReport generated: visual-report.md")
              
              # If regression test, compare with baseline
              if test_type == "regression" and Path("baseline-screenshots").exists():
                  differences = await compare_screenshots("baseline-screenshots", "screenshots")
                  print("\nRegression Test Results:")
                  for diff in differences:
                      print(f"- {diff['file']}: {diff['status']} (similarity: {diff['similarity']:.2%})")
          
          if __name__ == "__main__":
              asyncio.run(main())
          EOF
          
      - name: Run visual verification
        env:
          TEST_URL: ${{ inputs.test_url || 'https://asharitech-web.pages.dev' }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python visual_test.py "$TEST_URL" "${{ inputs.viewport_sizes }}" "${{ inputs.visual_test_type }}"
          
      - name: Upload screenshots
        uses: actions/upload-artifact@v4
        with:
          name: visual-screenshots-${{ github.run_id }}
          path: screenshots/
          retention-days: 30
          
      - name: Upload visual report
        uses: actions/upload-artifact@v4
        with:
          name: visual-report-${{ github.run_id }}
          path: visual-report.md
          retention-days: 30
          
      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('visual-report.md', 'utf8');
            
            // Truncate report if too long
            const maxLength = 65000;
            const truncatedReport = report.length > maxLength 
              ? report.substring(0, maxLength) + '\n\n... (truncated)'
              : report;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸŽ¨ Visual Verification Results\n\n${truncatedReport}\n\n[View full report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`
            });